{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model Size and Dataset Size Scaling Laws\n",
    "\n",
    "Reproducing Approach 3 from [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) to determine the optimal model size and dataset size for a decoder-only transformer model. Based on [scaling_laws.ipynb](https://github.com/karpathy/nanoGPT/blob/master/scaling_laws.ipynb) by Andrej Karpathy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and reprodiction of the results from Chinchilla - https://arxiv.org/pdf/2203.15556\n",
    "# Can be used to determine compute-optimal models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Use LaTeX style for text rendering\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Computer Modern']\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}\\usepackage{amsfonts}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_size(i: int) -> str:\n",
    "    \"\"\"Abbreviate numbers for logging, showing the three most significant digits.\"\"\"\n",
    "    for unit in ['', 'K', 'M', 'B', 'T', 'P']:\n",
    "        if i < 1000:\n",
    "            return f'{i:.3g}{unit}'\n",
    "        i /= 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcuating Expected Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model parameters as in GPT-alpha\n",
    "\n",
    "def gpt_alpha_params(T, V, C, H, N):\n",
    "    \"\"\"Calculate the total number of parameters for GPT alpha. No biases.\"\"\"\n",
    "    out = OrderedDict()\n",
    "    # Token and position embeddings\n",
    "    out['emebedding/position'] = C * T\n",
    "    out['embedding/token'] = C * V\n",
    "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
    "    # Attention blocks\n",
    "    out['attention/ln'] = C\n",
    "    out['attention/qkv'] = 3 * (C**2)\n",
    "    out['attention/proj'] = C**2\n",
    "    out['attention'] = out['attention/ln'] + out['attention/qkv'] + out['attention/proj']\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * C\n",
    "    out['mlp/ln'] = C\n",
    "    out['mlp/ffw'] = C * ffw_size\n",
    "    out['mlp/proj'] = ffw_size * C\n",
    "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
    "    # Transformer blocks\n",
    "    out['block'] = out['attention'] + out['mlp']\n",
    "    out['transformer'] = N * out['block']\n",
    "    out['ln_f'] = C\n",
    "    out['dense'] = 0 # Zero due to parameter sharing with the embedding layer\n",
    "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
    "    return out\n",
    "\n",
    "def gpt_alpha_flops(T, V, C, H, N):\n",
    "    \"\"\"Calculate the total number of FLOPs for GPT alpha. Only weight FLOPs are considered.\"\"\"\n",
    "    # Only weight FLOPs are considered as other operations (layernorm, softmax) are negligible\n",
    "    # Matrix multiplcation FLOPs are 2*M*N*P for (MxN) @ (NxP)\n",
    "    out = OrderedDict()\n",
    "    head_size = C // H\n",
    "    # Attention blocks\n",
    "    out['attention/qkv'] = 2 * T * (C * 3 * C) # Projection of Q, K, V\n",
    "    out['attention/scores'] = 2 * T * T * C # Q @ K\n",
    "    out['attention/reduce'] = 2 * H * (T * T * head_size) # V @ scores\n",
    "    out['attention/proj'] = 2 * T * (C * C) # Final projection\n",
    "    out['attention'] = sum(out['attention/'+k] for k in ['qkv', 'scores', 'reduce', 'proj'])\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * C\n",
    "    out['mlp/ffw1'] = 2 * T * (C * ffw_size)\n",
    "    out['mlp/ffw2'] = 2 * T * (ffw_size * C)\n",
    "    out['mlp'] = out['mlp/ffw1'] + out['mlp/ffw2']\n",
    "    # Transformer blocks\n",
    "    out['block'] = out['attention'] + out['mlp']\n",
    "    out['transformer'] = N * out['block']\n",
    "    out['dense'] = 2 * T * (C * V)\n",
    "    # Total FLOPs\n",
    "    out['forward'] = out['transformer'] + out['dense']\n",
    "    out['backward'] = 2 * out['forward'] # Estimate backward pass as twice the forward pass\n",
    "    out['total'] = out['forward'] + out['backward']\n",
    "    return out\n",
    "\n",
    "# block_size, vocab_size, n_embd, n_head, n_layer\n",
    "gpt_alpha = dict(T=1024, V=50257, C=768, H=12, N=12)\n",
    "\n",
    "params = gpt_alpha_params(**gpt_alpha)\n",
    "print(f'{\"Name\":25s}{\"Parameters\":12s}{\"Ratio (%)\":10s}')\n",
    "print(\"-\" * 45)\n",
    "for k, v in params.items():\n",
    "    print(f'{k:20s}{v:15,d}{(v/params[\"total\"] * 100):10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops = gpt_alpha_flops(**gpt_alpha)\n",
    "print(f'{\"Name\":25s}{\"FLOPs\":12s}{\"Ratio (%)\":10s}')\n",
    "print(\"-\" * 45)\n",
    "for k, v in flops.items():\n",
    "    print(f'{k:20s}{v:15,d}{(v/flops[\"forward\"] * 100):10.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this to the number of FLOPS as estimated by PaLM - https://arxiv.org/pdf/2204.02311\n",
    "\n",
    "def estimate_flops(T, V, C, H, N):\n",
    "    \"\"\"Estimate model flops utilisation (MFU) according to PaLM.\"\"\"\n",
    "    params = gpt_alpha_params(T, V, C, H, N)\n",
    "    # Non-embedding model parameters. Token embeddings are shared with the dense layer.\n",
    "    n_params = params['total'] - params['emebedding/position']\n",
    "    flops_per_token = 6 * n_params + 12 * N * H * (C // H) * T\n",
    "    return flops_per_token * T\n",
    "\n",
    "flops_estimate = estimate_flops(**gpt_alpha)\n",
    "\n",
    "print(f'Estimated FLOPs: {flops_estimate:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 * 16 # Using gradient accumulation to simulate a batch size of 512\n",
    "dt = 3.696 # Seconds per batch for single A100 SMX4 40GB GPU (using compiled PyTorch)\n",
    "flops_achieved = flops['total'] * (batch_size / dt)\n",
    "\n",
    "# A100 produces 312 TFLOPs of bfloat16 running on tensor cores\n",
    "# https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet.pdf\n",
    "a100_flops = 312e12\n",
    "\n",
    "mfu = flops_achieved / a100_flops\n",
    "# Model flops utilisation (MFU) as a percentage of the total FLOPs promised by the A100\n",
    "print(f'Fraction of A100 used: {mfu * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = params['total'] # Total number of parameters\n",
    "D = 40e9 # 40B tokens to process\n",
    "flops = a100_flops * 8 * mfu # Total FLOPs in an 8x A100 node\n",
    "total_flops = 6 * n_params * D # 6 FLOPs per parameter per token approximation\n",
    "train_time = total_flops / flops\n",
    "print(f'Total expected training time: {train_time / 3600:.2f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of each checkpoint, which is the total number of parameters and buffers\n",
    "params_bytes = params['total'] * 4 # 4 bytes per fp32\n",
    "params_bytes += 2 * params_bytes # AdamW has two buffers per parameter for stats\n",
    "print(f'Estimated checkpoint size: {abbr_size(params_bytes)[:-1]}GB')\n",
    "\n",
    "# Calculate the ratio of GPU memory used just for the parameters and buffers\n",
    "gpu_mem = 40e9 # 40 GB A100 GPU\n",
    "print(f'GPU memory usage: {params_bytes / gpu_mem * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not much at all. Most of the memory is used by activations and intermediate calculations for forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinchilla Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_params(T, V, C, H, N):\n",
    "    \"\"\"Calculate the total number of parameters for GPT models.\"\"\"\n",
    "    embd_pos = C * T\n",
    "    embd_tok = C * V\n",
    "    embd = embd_pos + embd_tok\n",
    "    # Attention blocks\n",
    "    att_ln = 2 * C\n",
    "    att_qkv = 3 * (C**2 + C)\n",
    "    att_proj = C**2 + C \n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * C\n",
    "    mlp_ln = 2 * C\n",
    "    mlp_ffw = C * ffw_size + ffw_size\n",
    "    mlp_proj = ffw_size * C + C\n",
    "    # Transformer blocks\n",
    "    attention = att_ln + att_qkv + att_proj\n",
    "    mlp = mlp_ln + mlp_ffw + mlp_proj\n",
    "    block = attention + mlp\n",
    "    transformer = N * block\n",
    "    # Final layer norm and fully connected layer\n",
    "    ln_f = 2 * C\n",
    "    dense = C * V\n",
    "    # Total number of parameters (excluding embeddings)\n",
    "    return transformer + ln_f + dense\n",
    "\n",
    "def chinchilla_params(T, V, C, H, N, ffw_size):\n",
    "    \"\"\"Calculate the total number of parameters for Chinchilla models. Chinchilla uses relative position embeddings.\"\"\"\n",
    "    # embd = C * V (token embedding only)\n",
    "    # Attention blocks\n",
    "    att_ln = 2 * C\n",
    "    att_qkv = 3 * (C**2 + C)\n",
    "    att_proj = C**2 + C\n",
    "    att_rel_pos = C**2 + 2 * C  # Relative keys, content bias, relative bias\n",
    "    # MLP blocks\n",
    "    mlp_ln = 2 * C\n",
    "    mlp_ffw = C * ffw_size + ffw_size\n",
    "    mlp_proj = ffw_size * C + C\n",
    "    # Transformer blocks\n",
    "    attention = att_ln + att_qkv + att_proj + att_rel_pos\n",
    "    mlp = mlp_ln + mlp_ffw + mlp_proj\n",
    "    block = attention + mlp\n",
    "    transformer = N * block\n",
    "    # Final layer norm and fully connected layer\n",
    "    ln_f = 2 * C\n",
    "    dense = C * V\n",
    "    # Total number of parameters (excluding embeddings)\n",
    "    return transformer + ln_f + dense\n",
    "\n",
    "# The difference between gpt_alpha_params() and gpt_params() is due to the additional biases in\n",
    "# the attention, feed-forward layers and layer normalisation. Furthermore, gpt_params() excludes\n",
    "# the position embeddings, which are included in gpt_alpha_params(). Token embeddings are simply\n",
    "# shared with the final fully connected layer.\n",
    "print(f'{gpt_params(**gpt_alpha):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinchilla_flops(T, V, C, H, N, ffw_size):\n",
    "    \"\"\"Calculate total number of FLOPs, see Chinchilla paper Appendix F.\"\"\" \n",
    "    key_size = T // H\n",
    "    embd = 2 * T * V * C\n",
    "    # Attention blocks\n",
    "    att_qkv = 2 * 3 * T * C * (key_size * H) # Q,K,V @ W\n",
    "    att_logits = 2 * T * T * (key_size * H) # K @ Q\n",
    "    att_softmax = 3 * H * T * T # 3 * is for subtract (max), exp, divide\n",
    "    att_value = 2 * T * T * (key_size * H) # softmax @ V\n",
    "    att_linear = 2 * T * (key_size * H) * C # Final linear layer\n",
    "    attention = att_qkv + att_logits + att_softmax + att_value + att_linear\n",
    "    # MLP blocks\n",
    "    mlp = 2 * T * (C * ffw_size + C * ffw_size)\n",
    "    # Logits\n",
    "    logits = 2 * T * C * V\n",
    "    # Forward pass FLOPs, unlike paper, embeddings and logits are included\n",
    "    forward_flops = embd + N * (attention + mlp) + logits\n",
    "    backward_flops = 2 * forward_flops # as in https://arxiv.org/abs/2001.08361\n",
    "    return forward_flops + backward_flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the results from Table A4 from Chinchilla paper\n",
    "# (N, C, ffw_size, H)\n",
    "models = [\n",
    "    [10, 640, 2560, 10],\n",
    "    [20, 1024, 4096, 16],\n",
    "    [24, 1280, 5120, 10],\n",
    "    [26, 1792, 7168, 14],\n",
    "    [28, 2048, 8192, 16],\n",
    "    [40, 3584, 14336, 28]\n",
    "]\n",
    "\n",
    "for N, C, ffw_size, H in models:\n",
    "    # Chinchilla models use a vocabulary size of 32k and a block size of 2048\n",
    "    config = dict(T=2048, V=32000, C=C, H=H, N=N, ffw_size=ffw_size)\n",
    "    flops = chinchilla_flops(**config)\n",
    "    n_params = chinchilla_params(**config)\n",
    "    print(f'{n_params:.3g} params, {flops:.3g} FLOPs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Fitting a parametric loss function - $L(|\\theta|,D)$ \n",
    "\n",
    "From Approach 3 of Chinchilla. Fitting a parametric loss function $L(|\\theta|,D)$ to approximate the final loss given the model size and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(n_params, D):\n",
    "    \"\"\"Approximate loss given n_params and D dataset size (in tokens), per Chinchilla paper.\"\"\"\n",
    "    E = 1.69 # Entropy of natural language, limit of infinite model on infinite data\n",
    "    A = 406.4\n",
    "    B = 410.7\n",
    "    alpha = 0.34\n",
    "    beta = 0.28\n",
    "    return A / (n_params ** alpha) + B / (D ** beta) + E\n",
    "\n",
    "# Model sizes from 10M to 100B\n",
    "ns = 10 ** np.arange(7, 11, step=2**-4)\n",
    "# Dataset sizes from 1B to 1T\n",
    "ds = 10 ** np.arange(9, 12, step=2**-4)\n",
    "\n",
    "# 2D contour plot of loss as a function of model size and dataset size\n",
    "plt.figure(figsize=(12, 8))\n",
    "loss2d = np.log10(np.array([[loss(n, d) for d in ds] for n in ns]))\n",
    "im1 = plt.imshow(loss2d, extent=[9, 12, 7, 11], origin='lower', alpha=0.5, cmap='plasma', aspect='auto')\n",
    "cs1 = plt.contour(loss2d, levels=30, extent=[9, 12, 7, 11], origin='lower', colors='white', linewidths=1, alpha=0.8)\n",
    "# GPT-alpha model\n",
    "plt.scatter(np.log10(40e9), np.log10(124e6), color='black', marker='x', s=100, label=f'GPT-$\\\\alpha$ ($124M$, $40$B)')\n",
    "# Optimal model\n",
    "plt.scatter(np.log10(14.4e9), np.log10(365e6), color='black', marker='^', s=100, label=f'Optimal Model ($365M$, $14.4$B)')\n",
    "plt.clabel(cs1, inline=1, fontsize=14, fmt='%1.1f')\n",
    "plt.xlabel(r'$\\log_{10}(D)$')\n",
    "plt.ylabel(r'$\\log_{10}(|\\theta|)$')\n",
    "plt.legend(loc='upper right')\n",
    "cbar1 = plt.colorbar(im1, label=r'$\\log_{10}(\\text{loss})$', pad=0.02, aspect=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/loss_contour.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contour plot of compute as a function of model size and dataset size\n",
    "plt.figure(figsize=(12, 8))\n",
    "compute2d = np.log10(np.array([[6*n*d for d in ds] for n in ns]))\n",
    "im2 = plt.imshow(compute2d, extent=[9, 12, 7, 11], origin='lower', alpha=0.5, cmap='plasma', aspect='auto')\n",
    "cs2 = plt.contour(compute2d, levels=30, extent=[9, 12, 7, 11], origin='lower', colors='white', linewidths=1, alpha=0.8)\n",
    "# GPT-alpha model\n",
    "plt.scatter(np.log10(40e9), np.log10(124e6), color='black', marker='x', s=100, label=f'GPT-$\\\\alpha$ ($124M$, $40$B)')\n",
    "# Optimal model\n",
    "plt.scatter(np.log10(14.4e9), np.log10(365e6), color='black', marker='^', s=100, label=f'Optimal Model ($365M$, $14.4$B)')\n",
    "plt.clabel(cs2, inline=1, fontsize=14, fmt='%1.1f')\n",
    "plt.xlabel(r'$\\log_{10}(D)$')\n",
    "plt.ylabel(r'$\\log_{10}(|\\theta|)$')\n",
    "plt.legend(loc='upper right')\n",
    "cbar2 = plt.colorbar(im2, label=r'$\\log_{10}(\\text{flops})$', pad=0.02, aspect=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/compute_contour.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLOPs from 10^18 to 10^23\n",
    "fs = 10 ** np.arange(18, 24, step=2**-4)\n",
    "\n",
    "# Compute corresponding D values for each combination of FLOPs and n_params\n",
    "ds_flops = np.array([[flops / (6 * n) for flops in fs] for n in ns])\n",
    "\n",
    "# 2D contour plot of loss as a function of model size and FLOPs budget\n",
    "plt.figure(figsize=(12, 8))\n",
    "loss2d_flops = np.log10(np.array([[loss(n, d) for d in ds] for n, ds in zip(ns, ds_flops)]))\n",
    "im1 = plt.imshow(loss2d_flops, extent=[18, 23, 7, 11], origin='lower', alpha=0.5, cmap='plasma', aspect='auto')\n",
    "cs1 = plt.contour(loss2d_flops, levels=30, extent=[18, 23, 7, 11], origin='lower', colors='white', linewidths=1, alpha=0.8)\n",
    "# GPT-alpha model\n",
    "plt.scatter(np.log10(3.16e19), np.log10(124e6), color='black', marker='x', s=100, label=f'GPT-$\\\\alpha$ ($124M$)')\n",
    "# Optimal model\n",
    "plt.scatter(np.log10(3.16e19), np.log10(365e6), color='black', marker='^', s=100, label=f'Optimal Model ($365M$)')\n",
    "plt.clabel(cs1, inline=1, fontsize=14, fmt='%1.1f')\n",
    "plt.xlabel(r'$\\log_{10}(\\text{FLOPs})$')\n",
    "plt.ylabel(r'$\\log_{10}(|\\theta|)$')\n",
    "plt.legend(loc='upper right')\n",
    "cbar1 = plt.colorbar(im1, label=r'$\\log_{10}(\\text{loss})$', pad=0.02, aspect=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/loss_contour_flops.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any $(|\\theta|, D)$, the loss and total flops can be estimated. Therefore, given a specific budget of flops $C$, it is possible to find the optimal $$(|\\theta|^*, D^*) = \\text{argmin}_{\\text{FLOPs}(|\\theta|, D) = C} L(|\\theta|, D)$$ that minimises the loss. Therefore, the optimal model size and dataset size can be determined for a given compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8x A100 SMX4 40GB GPUs produce a theoretical maximum of 2.50e15 FLOPs using BF16\n",
    "# The MFU is 0.3885, which is the fraction of the total FLOPs used by the model,\n",
    "# giving an expected FLOPs budget of 9.70e14 FLOPs. Over a training time of 12 hours,\n",
    "# this produces 9.70e14 * 12 * 60 * 60 = 4.19e19 total FLOPs. Note that this has been\n",
    "# reduced to 3.16e19 FLOPs to account for overhead in training time with validation.\n",
    "\n",
    "C = 3.16e19 # Example compute budget\n",
    "# Range of model sizes to consider from 10M to 100B\n",
    "ns = 10 ** np.arange(7, 11, step=2**-4)\n",
    "# Compute D for each model size using C = 6 * n_params * D\n",
    "ds = C / (6 * ns)\n",
    "# Calculate the loss for each model size and dataset size\n",
    "losses = loss(ns, ds)\n",
    "# Find the optimal model size\n",
    "best = np.argmin(losses)\n",
    "print(f'Optimal model size: {abbr_size(ns[best])} params, {abbr_size(ds[best])} tokens')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
