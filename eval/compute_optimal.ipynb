{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute-Optimal Models\n",
    "\n",
    "Reproducing Approach 3 from [Chinchilla](https://arxiv.org/pdf/2203.15556.pdf) to determine the optimal model size and dataset size for a decoder-only transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and reprodiction of the results from Chinchilla - https://arxiv.org/pdf/2203.15556.pdf\n",
    "# Can be used to determine compute-optimal models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use LaTeX style for text rendering\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Computer Modern']\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}\\usepackage{amsfonts}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbr_size(i: int) -> str:\n",
    "    \"\"\"Abbreviate numbers for logging, showing the three most significant digits.\"\"\"\n",
    "    for unit in ['', 'K', 'M', 'B', 'T', 'P']:\n",
    "        if i < 1000:\n",
    "            return f'{i:.3g}{unit}'\n",
    "        i /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_params(T, V, C, H, N):\n",
    "    \"\"\"Calculate the total number of parameters for GPT models.\"\"\"\n",
    "    # embeddings = C * V + C * T\n",
    "    # Feed-forward network size\n",
    "    ffw_size = 4 * C\n",
    "    # Transformer blocks\n",
    "    attention = 3 * C**2 + 3 * C # Attention weights and biases\n",
    "    attproj = C**2 + C # Attention output projection\n",
    "    ffw = C * ffw_size + ffw_size # Feed-forward weights and biases\n",
    "    ffwproj = ffw_size * C + C # Feed-forward output projection\n",
    "    layernorms = 2 * 2 * C # 2 layer norms per block\n",
    "    # Final layer norm and fully connected layer\n",
    "    ln_f = 2 * C\n",
    "    dense = C * V\n",
    "    # Total number of parameters (excluding embeddings)\n",
    "    return  N * (attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "\n",
    "def chinchilla_params(T, V, C, H, N, ffw_size):\n",
    "    \"\"\"Calculate the total number of parameters for Chinchilla models.\"\"\"\n",
    "    # embeddings = C * V (token embedding only)\n",
    "    # Transformer blocks\n",
    "    attention = 3 * C**2 + 3 * C  # Attention weights and biases\n",
    "    relative_pos = C**2 + 2 * C  # Relative keys, content bias, relative bias\n",
    "    attproj = C**2 + C  # Attention output projection\n",
    "    ffw = C * ffw_size + ffw_size  # Feed-forward weights and biases\n",
    "    ffwproj = ffw_size * C + C  # Feed-forward output projection\n",
    "    layernorms = 2 * 2 * C  # 2 layer norms per block\n",
    "    # Final layer norm and fully connected layer\n",
    "    ln_f = 2 * C\n",
    "    dense = C * V\n",
    "    # Total number of parameters (excluding embeddings)\n",
    "    return N * (attention + relative_pos + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "\n",
    "# block_size, vocab_size, n_embd, n_head, n_layer\n",
    "gpt_alpha = dict(T = 1024, V = 50257, C = 768, H = 12, N = 12)\n",
    "gpt_params(**gpt_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinchilla_flops(T, V, C, H, N, ffw_size):\n",
    "    \"\"\"Calculate total number of FLOPs, see Chinchilla paper Appendix F.\"\"\" \n",
    "    key_size = T // H\n",
    "    # embeddings = 2 * T * V * C\n",
    "    # Q,K,V projections\n",
    "    attention = 2 * 3 * T * C * (key_size * H)\n",
    "    # K @ Q logits\n",
    "    attlogits = 2 * T * T * (key_size * H)\n",
    "    # Softmax\n",
    "    attsoftmax = 3 * H * T * T # 3 * is for subtract (max), exp, divide (?)\n",
    "    # Softmax @ V reductions\n",
    "    attvalue = 2 * T * T * (key_size * H)\n",
    "    # Final linear\n",
    "    attlinear = 2 * T * (key_size * H) * C\n",
    "    att = attention + attlogits + attsoftmax + attvalue + attlinear\n",
    "    # Feed-forward\n",
    "    dense = 2 * T * (C * ffw_size + C * ffw_size)\n",
    "    # logits = 2 * T * C * V\n",
    "    # Forward pass FLOPs, paper does not include embeddings and logits\n",
    "    forward_flops = N * (att + dense)\n",
    "    backward_flops = 2 * forward_flops # as in https://arxiv.org/abs/2001.08361\n",
    "    return forward_flops + backward_flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the results from Table A4 from Chinchilla paper Appendix\n",
    "# (N, C, ffw_size, H)\n",
    "models = [\n",
    "    [10, 640, 2560, 10],\n",
    "    [20, 1024, 4096, 16],\n",
    "    [24, 1280, 5120, 10],\n",
    "    [26, 1792, 7168, 14],\n",
    "    [28, 2048, 8192, 16],\n",
    "    [40, 3584, 14336, 28]\n",
    "]\n",
    "\n",
    "for N, C, ffw_size, H in models:\n",
    "    # Chinchilla models use a vocabulary size of 32k and a block size of 2048\n",
    "    config = dict(T=2048, V=32000, C=C, H=H, N=N, ffw_size=ffw_size)\n",
    "    flops = chinchilla_flops(**config)\n",
    "    n_params = chinchilla_params(**config)\n",
    "    print(f'{n_params:.3g} params, {flops:.3g} FLOPs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Fitting a parametric loss function - $L(n_{\\text{params}},D)$ \n",
    "\n",
    "Fitting a function $L(n_{\\text{params}},D)$ to approximate the final loss given the model size and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(n_params, D):\n",
    "    \"\"\"Approximate loss given n_params and D dataset size (in tokens), per Chinchilla paper.\"\"\"\n",
    "    E = 1.69 # Entropy of natural language, limit of infinite model on infinite data\n",
    "    A = 406.4\n",
    "    B = 410.7\n",
    "    alpha = 0.34\n",
    "    beta = 0.28\n",
    "    return A / (n_params ** alpha) + B / (D ** beta) + E\n",
    "\n",
    "# Model sizes from 10M to 100B\n",
    "ns = 10 ** np.arange(7, 11, step=2**-4)\n",
    "# Dataset sizes from 1B to 1T\n",
    "ds = 10 ** np.arange(9, 12, step=2**-4)\n",
    "\n",
    "# 2D contour plot of loss as a function of model size and dataset size\n",
    "plt.figure(figsize=(12, 8))\n",
    "loss2d = np.log10(np.array([[loss(n, d) for d in ds] for n in ns]))\n",
    "im1 = plt.imshow(loss2d, extent=[9, 12, 7, 11], origin='lower', alpha=0.5, cmap='plasma', aspect='equal')\n",
    "cs1 = plt.contour(loss2d, levels=30, extent=[9, 12, 7, 11], origin='lower', colors='white', linewidths=1, alpha=0.8)\n",
    "plt.clabel(cs1, inline=1, fontsize=8, fmt='%1.1f')\n",
    "plt.xlabel(r'$\\log_{10}(D)$')\n",
    "plt.ylabel(r'$\\log_{10}(n_{\\text{params}})$')\n",
    "cbar1 = plt.colorbar(im1, label=r'$\\log_{10}(\\text{loss})$', pad=0.02, aspect=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/compute_contour.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2D contour plot of the compute as a function of model size and dataset size using FLOPs = 6 * n_params * D\n",
    "plt.figure(figsize=(12, 8))\n",
    "compute2d = np.log10(np.array([[6*n*d for d in ds] for n in ns]))\n",
    "im2 = plt.imshow(compute2d, extent=[9, 12, 7, 11], origin='lower', alpha=0.5, cmap='plasma', aspect='equal')\n",
    "cs2 = plt.contour(compute2d, levels=30, extent=[9, 12, 7, 11], origin='lower', colors='white', linewidths=1, alpha=0.8)\n",
    "plt.clabel(cs2, inline=1, fontsize=8, fmt='%1.1f')\n",
    "plt.xlabel(r'$\\log_{10}(D)$')\n",
    "plt.ylabel(r'$\\log_{10}(n_{\\text{params}})$')\n",
    "cbar2 = plt.colorbar(im2, label=r'$\\log_{10}(\\text{flops})$', pad=0.02, aspect=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/loss_contour.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any $(n_{\\text{params}}, D)$, the loss and total flops can be estimated. Therefore, given a specific budget of flops $C$, it is possible to find the optimal $$(n_{\\text{params}}^*, D^*) = \\argmin_{\\text{FLOPs}(n_{\\text{params}}, D) = C} L(n_{\\text{params}}, D)$$ that minimises the loss. Therefore, the optimal model size and dataset size can be determined for a given compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8x A100 SMX4 40GB GPUs produce 1.25e15 FLOPs using TF32 without sparsity\n",
    "# Over 12 hours, 1.25e15 * 12 * 60 * 60 = 5.39e19 total FLOPs\n",
    "\n",
    "C = 5.39e19 # Example compute budget\n",
    "# Range of model sizes to consider from 10M to 100B\n",
    "ns = 10 ** np.arange(7, 11, step=2**-4)\n",
    "# Compute D for each model size using C = 6 * n_params * D\n",
    "ds = C / (6 * ns)\n",
    "# Calculate the loss for each model size and dataset size\n",
    "losses = loss(ns, ds)\n",
    "# Find the optimal model size\n",
    "best = np.argmin(losses)\n",
    "print(f'Optimal model size: {abbr_size(ns[best])} params, {abbr_size(ds[best])} tokens')\n",
    "\n",
    "# Plot the optimal model size\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.semilogx(ns, losses, color='black', linewidth=1)\n",
    "plt.semilogx(ns[best], losses[best], color='black', marker='o', markersize=4, label='Optimal point')\n",
    "plt.xlabel(r'$n_{\\text{params}}$', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.grid(True, which='both', ls='-', alpha=0.2)\n",
    "plt.legend(fontsize=12)\n",
    "# Annotate the optimal point\n",
    "plt.annotate(f'{abbr_size(ns[best])}',\n",
    "             xy=(ns[best], losses[best]), xytext=(ns[best]*0.75, losses[best]*1.02),\n",
    "             fontsize=12, ha='left', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../cache/optimal_params.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models on the left of the optimal point are underparameterised and excessively trained. The models on the right of the optimal point are overparameterised and undertrained. The optimal point is the sweet spot where the model is neither underparameterised nor overparameterised."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
